{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9wUL_Fy5qUDI"},"source":["# UTSA CS 3793: Assignment-3\n","\n","**Last Name - First Name - (abc123)**\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NM8b9KVYsETT"},"source":["## Learning Objectives\n","\n","Implement 2 different machine learning algorithms\n","*   Stochastic Gradient Descent\n","*   ID3 Decision Tree\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJFbAqUYmvRw","executionInfo":{"status":"ok","timestamp":1699663517311,"user_tz":360,"elapsed":667,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}},"outputId":"bc9ac37d-8dfd-40b6-83e2-e331b7499925"},"execution_count":251,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"LzR4Ic34zJlT"},"source":["\n","## Description\n","\n","This assignment is focused on **machine learning**, mainly on the implementation of 2 different algorithms - Stochastic Gradient Descent & ID3 decision tree.\n","The assignment is divided into two sections, each for one unique ML algorithm.\n","\n","The base structure and comments are provided on what should be done. You can use some libraries that help support you for the successful completion of the assignment. However, you **CANNOT** use a complete library that contains the implementation of ML algorithms. You can get pieces of code from online, but please cite the source properly.\n"]},{"cell_type":"markdown","metadata":{"id":"vnPfmHAOteOI"},"source":["##Import Libraries\n","\n","Write all the import statements here. This should be for both algorithm implmentations. As mentioned before, you can not use any premade ML libraries."]},{"cell_type":"code","metadata":{"id":"f9apbZGptej6","executionInfo":{"status":"ok","timestamp":1699663517311,"user_tz":360,"elapsed":2,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["from google.colab import drive\n","import numpy as np\n","import pandas as pd\n","import random\n","from math import exp\n","import matplotlib.pyplot as plt\n","import math\n"],"execution_count":252,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdqXyFZ95P0j","executionInfo":{"status":"ok","timestamp":1699663517311,"user_tz":360,"elapsed":1,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Assume that the data files are in the following folder -- THIS WILL BE USED BY THE TA\n","basePath = \"/content/drive/My Drive/Colab Notebooks/Artificial Intelligence/Data/\"\n"],"execution_count":253,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeYRnesWqvLm"},"source":["#Stochastic Gradient Descent\n","\n","In this section, you will implement the Stochastic Gradient Descent algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that you are given **all binary-valued attributes** and that there are **no missing values** in the train or test data.\n"]},{"cell_type":"markdown","metadata":{"id":"IUVZIK6ctMi4"},"source":["##Algorithm\n","\n","(40 points)\n","\n","Following are the data files that will be provided to you for the gradient descent algorithm implementation.\n","\n","*   Training file - 'gd-train.dat'\n","*   Testing file - 'gd-test.dat'\n","\n","In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are different example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.\n","\n","Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments.\n","\n"]},{"cell_type":"code","metadata":{"id":"2XoSqVJG5FkG","executionInfo":{"status":"ok","timestamp":1699663517311,"user_tz":360,"elapsed":1,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Data file name variables\n","basePath = \"/content/drive/My Drive/Colab Notebooks/Artificial Intelligence/Data/\"\n","train_path = basePath + \"gd-train.dat\"\n","test_path = basePath + \"gd-test.dat\"\n","\n","\n"],"execution_count":254,"outputs":[]},{"cell_type":"code","metadata":{"id":"l73PTZtCPdxj","executionInfo":{"status":"ok","timestamp":1699663517442,"user_tz":360,"elapsed":132,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Function to read data from a file\n","'''def read_data(file_path):\n","  # Print the first few lines of the file to understand its format\n","  with open(train, 'r') as file:\n","    for _ in range(5):\n","      print(file.readline().strip())\n","\n","  with open(file_path, 'r') as file:\n","    lines = file.readlines()\n","\n","  attributes = lines[0].strip().split('\\t')\n","  data = []\n","  for line in lines[1:]:\n","    if line.strip():  # Check if the line is not empty\n","      instance = list(map(int, line.strip().split('\\t')))\n","      data.append(instance)\n","\n","  return attributes, data\n","\n","# Read the training and testing data files\n","train_data = read_data(train)\n","test_data = read_data(test)\n","'''\n","def read_data(file_path):\n","  with open(file_path, 'r') as file:\n","    lines = file.readlines()\n","\n","  # Remove empty lines\n","  lines = [line.strip() for line in lines if line.strip()]\n","\n","  attributes = lines[0].split('\\t')\n","  data = [list(map(int, line.split('\\t'))) for line in lines[1:]]\n","\n","  return attributes, data\n"],"execution_count":255,"outputs":[]},{"cell_type":"code","metadata":{"id":"VN3FC2qcPfcR","executionInfo":{"status":"ok","timestamp":1699663517443,"user_tz":360,"elapsed":8,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Activation Function - implement Sigmoid\n","def activation_function(h):\n","    return 1 / (1 + exp(-h))\n","    # given 'h' compute and return 'z' based on the activation function implemented\n",""],"execution_count":256,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5njAQvmPmOC","executionInfo":{"status":"ok","timestamp":1699663517443,"user_tz":360,"elapsed":7,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["def train(train_data, learning_rate=0.05):\n","  num_attributes = len(train_data[0]) - 1\n","  weights = [0.0] * num_attributes  # Initialize weights to floats\n","  weights.append(0.0)  # Bias weight\n","\n","  for instance in train_data:\n","      x = [float(xi) for xi in instance[:-1]]  # Convert features to floats\n","      y = instance[-1]\n","\n","      h = sum(w * xi for w, xi in zip(weights, x))\n","      z = activation_function(h)\n","\n","      # Update weights\n","      for i in range(num_attributes):\n","          weights[i] += learning_rate * (y - z) * x[i]\n","\n","      # Update bias weight\n","      weights[-1] += learning_rate * (y - z)\n","  return weights\n","\n","\n"],"execution_count":257,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gx98kNxDPq5B","executionInfo":{"status":"ok","timestamp":1699663517443,"user_tz":360,"elapsed":7,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Test the model (weights learnt) using the given test dataset\n","# return the accuracy value\n","# Test the model using the given test dataset\n","def test(test_data, weights, threshold):\n","  correct_predictions = 0\n","  for instance in test_data:\n","    x = np.array([float(xi) for xi in instance[:-1]])  # Convert features to floats\n","    y = instance[-1]\n","\n","    h = np.dot(weights[:-1], x) + weights[-1]\n","    z = activation_function(h)\n","\n","    predicted_class = 1 if z >= threshold else 0\n","\n","    if predicted_class == y:\n","      correct_predictions += 1\n","\n","  accuracy = correct_predictions / len(test_data)\n","  return accuracy\n","\n","\n",""],"execution_count":258,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5B6IG0APxH4","executionInfo":{"status":"ok","timestamp":1699663517443,"user_tz":360,"elapsed":7,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["def gradient_descent(train_data, test_data, learning_rate=0.05, threshold=0.5):\n","  # Transpose the data\n","  num_epochs = 100\n","  train_data = list(map(list, zip(*train_data)))\n","  test_data = list(map(list, zip(*test_data)))\n","\n","  # Call the train function to train the model and obtain the weights\n","  weights = train(train_data, learning_rate)\n","\n","  # call the test function with the training dataset to obtain the training accuracy\n","  train_accuracy = test(train_data, weights, threshold)\n","\n","  # call the test function with the testing dataset to obtain the testing accuracy\n","  test_accuracy = test(test_data, weights, threshold)\n","  for epoch in range(num_epochs):\n","      for instance in train_data:\n","        # Modify this line to handle non-numeric values\n","        x = [float(xi) if xi.replace('.', '', 1).isdigit() else 0.0 for xi in instance[:-1]]\n","        y = instance[-1]\n","\n","        h = sum(w * xi for w, xi in zip(weights, x))\n","        z = activation_function(h)\n","\n","\n","  return train_accuracy, test_accuracy\n","\n","# Read the training and testing data files\n","train_data = read_data(train_path)\n","test_data = read_data(test_path)"],"execution_count":259,"outputs":[]},{"cell_type":"code","metadata":{"id":"MB_b5ButP3w3","executionInfo":{"status":"ok","timestamp":1699663517443,"user_tz":360,"elapsed":7,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Threshold of 0.5 will be used to classify the instance for the test. If the value is >= 0.5, classify as 1 or else 0.\n","threshold = 0.5\n"],"execution_count":260,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdPwgSBOtb1P","colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"status":"error","timestamp":1699663517444,"user_tz":360,"elapsed":8,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}},"outputId":"db1a6442-0393-420a-afd1-34a7dde6a93f"},"source":["# Main algorithm loop\n","learning_rates = [0.05, 0.1]\n","for learning_rate in learning_rates:\n","  # For each learning rate selected, call the gradient descent function\n","  # to obtain the train and test accuracy values\n","  train_acc, test_acc = gradient_descent(train_data, test_data, learning_rate, threshold)\n","\n","  # Print both the accuracy values\n","  print(f\"Accuracy for LR of {learning_rate} on Training set = {train_acc * 100:.2f}%\")\n","  print(f\"Accuracy for LR of {learning_rate} on Testing set = {test_acc * 100:.2f}%\")"],"execution_count":261,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-261-5d812d230c9d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# For each learning rate selected, call the gradient descent function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# to obtain the train and test accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Print both the accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-259-2fe509170f87>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(train_data, test_data, learning_rate, threshold)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Call the train function to train the model and obtain the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# call the test function with the training dataset to obtain the training accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-257-b518c3ae743a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Convert features to floats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-257-b518c3ae743a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Convert features to floats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'A1'"]}]},{"cell_type":"markdown","metadata":{"id":"lYxmgcnes9cS"},"source":["##Extra Credit - Accuracy Plots\n","\n","(05 points)\n","\n","Use the above accuracy results on the training and testing data and write code to plot the graphs as mentioned in the code block below.\n","\n"]},{"cell_type":"code","metadata":{"id":"fbBNakSDq0Wv","executionInfo":{"status":"aborted","timestamp":1699663517444,"user_tz":360,"elapsed":6,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Plot the graphs for accuracy results.\n","# There will be 2 graphs - one for training data and the other for testing data\n","# For each graph,\n","    # X-axis will be the learning rate going from 0.05-1 in increments on 0.05\n","    # Y-axis will be the accuracy values at the selected learning rate.\n","# Plot the graphs for accuracy results\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Function to plot accuracy graphs\n","def plot_accuracy_graphs(learning_rates, train_accuracies, test_accuracies):\n","    plt.plot(learning_rates, train_accuracies, marker='o', label='Training Data')\n","    plt.plot(learning_rates, test_accuracies, marker='o', label='Testing Data')\n","    plt.title('Accuracy vs Learning Rate')\n","    plt.xlabel('Learning Rate')\n","    plt.ylabel('Accuracy (%)')\n","    plt.legend()\n","    plt.show()\n","\n","# Data file name variables\n","basePath = \"/content/drive/My Drive/Colab Notebooks/Artificial Intelligence/Data/\"\n","train_path = basePath + \"gd-train.dat\"\n","test_path = basePath + \"gd-test.dat\"\n","\n","# Read the training and testing data files\n","train_data = read_data(train_path)\n","test_data = read_data(test_path)\n","\n","# Threshold of 0.5 will be used to classify the instance for the test.\n","# If the value is >= 0.5, classify as 1; else 0.\n","threshold = 0.5\n","\n","# Generate a range of learning rates from 0.05 to 1 in increments of 0.05\n","learning_rates = np.arange(0.05, 1.05, 0.05)\n","\n","# Initialize empty lists to store accuracy values\n","train_accuracies = []\n","test_accuracies = []\n","\n","# Main algorithm loop\n","for learning_rate in learning_rates:\n","    # For each learning rate selected, call the gradient descent function\n","    # to obtain the train and test accuracy values\n","    train_acc, test_acc = gradient_descent(train_data, test_data, learning_rate, threshold)\n","\n","    # Append accuracy values to the lists\n","    train_accuracies.append(train_acc * 100)\n","    test_accuracies.append(test_acc * 100)\n","\n","# Plot the accuracy graphs\n","plot_accuracy_graphs(learning_rates, train_accuracies, test_accuracies)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"onnqJYTEq0l3"},"source":["#ID3 Decision Tree\n","\n","In this section, you will implement the ID3 Decision Tree algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that there are **no missing values** in the train or test data.\n"]},{"cell_type":"markdown","metadata":{"id":"eDNztBkTtRPw"},"source":["## Algorithm\n","\n","(85 points)\n","\n","Following are the data files that will be provided to you for the ID3 algorithm implementation.\n","\n","*   Training file - 'id3-train.dat'\n","*   Testing file - 'id3-test.dat'\n","\n","In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.\n","\n","In a decision tree, if you reach a leaf node but still have examples that belong to different classes, then choose the most frequent class (among the instances at the leaf node). If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes, then choose the class that is most frequent in the entire training set. You do not need to implement pruning. Also, don’t forget to use logarithm base 2 when computing entropy and set (0 log 0) to 0.\n","\n","Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments. The code should output the following 3 things:\n","\n","*   Print the Decision Tree created, in the following example format:\n","\n","    ```\n","    attr1 = 0 :\n","        attr2 = 0 :\n","            attr3 = 0 : 1 -- 4\n","            attr3 = 1 : 0 -- 9\n","        attr2 = 1 :\n","            attr4 = 0 : 0 -- 2\n","            attr4 = 1 : 1 -- 10\n","    attr1 = 1 :\n","        attr2 = 1 : 1 -- 17\n","\n","    ```\n","\n","*   Accuracy on the Training data = x %\n","*   Accuracy on the Test data = x %\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"wTLz1lGYQJKS","executionInfo":{"status":"aborted","timestamp":1699663517444,"user_tz":360,"elapsed":6,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Data file name variables\n","train = basePath + \"id3-train.dat\"\n","test = basePath + \"id3-test.dat\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwHYC6xjQP5D","executionInfo":{"status":"aborted","timestamp":1699663517444,"user_tz":360,"elapsed":6,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Pseudocode for the ID3 algorithm. Use this to create function(s).\n","# def ID3(data, root, attributesRemaining):\n","    # If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes\n","        # Choose and the class that is most frequent in the entire training set and return the updated tree\n","    # If all the instances have only one class label\n","        # Make this as the leaf node and use the label as the class value of the node and return the updated tree\n","    # If you reached a leaf node but still have examples that belong to different classes (there are no remaining attributes to be split)\n","        # Assign the most frequent class among the instances at the leaf node and return the updated tree\n","    # Find the best attribute to split by calculating the maximum information gain from the attributes remaining by calculating the entropy\n","    # Split the tree using the best attribute and recursively call the ID3 function using DFS to fill the sub-tree\n","    # return the root as the tree\n","    # Data file name variables\n","\n","\n","def calculate_entropy(class_counts):\n","  total_instances = sum(class_counts)\n","  entropy = 0\n","  for count in class_counts:\n","    if count != 0:\n","      probability = count / total_instances\n","      entropy -= probability * math.log2(probability)\n","  return entropy\n","\n","def ID3(data, root, attributes_remaining):\n","    # If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes\n","    if len(set(data[:, -1])) == 1:\n","      root.value = data[0, -1]\n","      return root\n","\n","    # If all the instances have only one class label\n","    if not any(attributes_remaining):\n","      root.value = max(set(data[:, -1]), key=list(data[:, -1]).count)\n","      return root\n","\n","    # Find the best attribute to split by calculating the maximum information gain\n","    best_attribute = find_best_attribute(data, attributes_remaining)\n","\n","    # Split the tree using the best attribute\n","    root.value = best_attribute\n","    remaining_attributes = attributes_remaining.copy()\n","    remaining_attributes[best_attribute] = False\n","\n","    for value in set(data[:, best_attribute]):\n","      new_node = Node(parent=root, value=value)\n","      root.children[value] = ID3(data[data[:, best_attribute] == value][:, :-1], new_node, remaining_attributes)\n","\n","    return root\n","\n","def find_best_attribute(data, attributes_remaining):\n","  class_counts_all = [sum(data[:, -1] == c) for c in set(data[:, -1])]\n","  entropy_all = calculate_entropy(class_counts_all)\n","\n","  best_attribute = None\n","  max_information_gain = 0\n","\n","  for attribute in range(len(attributes_remaining)):\n","    if attributes_remaining[attribute]:\n","      information_gain = entropy_all - calculate_attribute_entropy(data, attribute)\n","      if information_gain > max_information_gain:\n","        max_information_gain = information_gain\n","        best_attribute = attribute\n","\n","  return best_attribute\n","\n","def calculate_attribute_entropy(data, attribute):\n","  attribute_values = set(data[:, attribute])\n","  entropy = 0\n","\n","  for value in attribute_values:\n","    subset = data[data[:, attribute] == value]\n","    class_counts = [sum(subset[:, -1] == c) for c in set(subset[:, -1])]\n","    entropy += (len(subset) / len(data)) * calculate_entropy(class_counts)\n","\n","  return entropy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XELGzRDftS77","executionInfo":{"status":"aborted","timestamp":1699663517444,"user_tz":360,"elapsed":6,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Following is the base code structure. Feel free to change the code structure as you see fit, maybe even create more functions.\n","\n","# Read the first line in the training data file, to get the number of attributes\n","# Read all the training instances and the ground truth class labels.\n","# Create the decision tree by implementing the ID3 algorithm. Pseudocode provided above.\n","# Print the tree in the example format mentioned.\n","# Use the above created tree to predict the training data and print the accuracy as \"Accuracy on the Training data = x %\"\n","    # For each training instance, predict the output label\n","    # Compare it with the ground truth class label and calculate the accuracy accordingly\n","# Use the above created tree to predict the testing data and print the accuracy as \"Accuracy on the Test data = x %\"\n","    # For each testing instance, predict the output label\n","    # Compare it with the ground truth class label and calculate the accuracy accordingly\n","# Define a Node class to represent the decision tree structure\n","\n","class Node:\n","  def __init__(self, parent=None, value=None):\n","    self.parent = parent\n","    self.value = value\n","    self.children = {}\n","\n","# Data file name variables\n","train_id3 = basePath + \"id3-train.dat\"\n","test_id3 = basePath + \"id3-test.dat\"\n","\n","# Read the training data for the ID3 Decision Tree\n","with open(train_id3, 'r') as file:\n","    # Skip the first line containing attribute names\n","    file.readline()\n","    # Read the rest of the lines as instances\n","    data_id3 = np.array([list(map(int, line.strip().split('\\t'))) for line in file])\n","\n","num_attributes_id3 = len(data_id3[0]) - 1\n","root_node_id3 = Node()\n","decision_tree_id3 = ID3(data_id3, root_node_id3, [True] * num_attributes_id3)\n","\n","def print_decision_tree(node, indent=\"\"):\n","    if node.value is not None:\n","        print(f\"{indent}Attribute {node.value} : {node.parent.value} -- {len(data_id3)}\")\n","    for value, child in node.children.items():\n","        print(f\"{indent} {value} : \", end=\"\")\n","        print_decision_tree(child, indent + \" \")\n","\n","print_decision_tree(decision_tree_id3)\n","\n","def predict(tree, instance):\n","    if not tree.children:\n","        return tree.value\n","    attribute_value = instance[tree.value]\n","    if attribute_value in tree.children:\n","        return predict(tree.children[attribute_value], instance)\n","    else:\n","        return max(tree.children.values(), key=lambda x: len(data_id3) if x.value is None else list(data_id3[:, -1]).count(x.value)).value\n","\n","# Use the ID3 decision tree to predict the training data\n","predicted_labels_id3_train = [predict(decision_tree_id3, instance) for instance in data_id3]\n","\n","# Calculate the accuracy on the Training data for the ID3 Decision Tree\n","accuracy_id3_train = sum(predicted_labels_id3_train == data_id3[:, -1]) / len(data_id3) * 100\n","print(f\"Accuracy on the Training data for ID3 Decision Tree = {accuracy_id3_train:.2f} %\")\n","\n","# Read the testing data for the ID3 Decision Tree\n","with open(test_id3, 'r') as file:\n","    file.readline()\n","    data_test_id3 = np.array([[int(value) for value in line.strip()] for line in file])\n","\n","# Use the ID3 decision tree to predict the testing data\n","predicted_labels_id3_test = [predict(decision_tree_id3, instance) for instance in data_test_id3]\n","\n","# Calculate the accuracy on the Test data for the ID3 Decision Tree\n","accuracy_id3_test = sum(predicted_labels_id3_test == data_test_id3[:, -1]) / len(data_test_id3) * 100\n","print(f\"Accuracy on the Test data for ID3 Decision Tree = {accuracy_id3_test:.2f} %\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvYowzzA4vcd"},"source":["##Extra Credit - Learning Curve\n","\n","(05 points)\n","\n","Instead of taking the entire training data (all 800 instances), loop through to select 'x' instances in the increments of 40 (i.e. 40, 80, 120, and so on). For each selected number 'x', randomly pick the example instances from the training data and call the ID3 function to create the decision tree. Calculate the accuracy of the created ID3 tree on the Test data file. Plot the corresponding graph, aka Learning Curve.\n"]},{"cell_type":"code","metadata":{"id":"FYSK99zp5a7H","executionInfo":{"status":"aborted","timestamp":1699663517444,"user_tz":360,"elapsed":856,"user":{"displayName":"Kendall Ramos","userId":"01944009309830321758"}}},"source":["# Loop through to select the number of instances 'x' in increments of 40\n","# For each 'x',\n","    # Randomly select 'x' instances\n","    # Create the ID3 decision tree using those instances\n","    # Calculate the accuracy of the ID3 tree created on the Test data\n","\n","# Plot the learning curve using the accuracy values\n","    # X-axis will be the number of training instances used for creating the tree\n","    # Y-axis will be the accuracy in % on the Test data\n","\n","\n","# Initialize variables for learning curve\n","# Learning Curve Section\n","learning_curve_x = []\n","learning_curve_y = []\n","\n","for num_instances in range(40, len(data_id3) + 1, 40):\n","  selected_instances = random.sample(data_id3, num_instances)\n","  root_node_id3 = Node()\n","  decision_tree_id3 = ID3(selected_instances, root_node_id3, [True] * num_attributes_id3)\n","  predicted_labels_test_id3 = [predict(decision_tree_id3, instance) for instance in data_test_id3]\n","  accuracy_test_id3 = sum(predicted_labels_test_id3 == data_test_id3[:, -1]) / len(data_test_id3) * 100\n","  learning_curve_x.append(num_instances)\n","  learning_curve_y.append(accuracy_test_id3)\n","\n","plt.plot(learning_curve_x, learning_curve_y, marker='o')\n","plt.title('ID3 Learning Curve')\n","plt.xlabel('Number of Training Instances')\n","plt.ylabel('Accuracy on Test Data (%)')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJSFgNBQrhQU"},"source":["#Submission Instructions\n","\n","1.   Complete all tasks above - **File MUST contain the output for ALL cells**\n","2.   Export this notebook as .ipynb\n","      (File > Download as ipynb)\n","3.   Upload the .ipynb file on Blackboard"]},{"cell_type":"markdown","metadata":{"id":"0lGvLE9H6ptL"},"source":["##Rubric\n","\n","*   (40 points) Gradient Descent Algorithm\n","*   (05 points) Extra Credit - GD Accuracy Plots\n","*   (85 points) ID3 Algorithm\n","*   (05 points) Extra Credit - ID3 Learning Curve\n"]}]}